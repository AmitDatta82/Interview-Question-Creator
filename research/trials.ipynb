{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GenAIProj\\\\MyProjs\\\\Interview-Question-Creator'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # type: ignore\n",
    "\n",
    "file_path=\"data/Java_Cheat_Sheet.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_gen = \"\"\n",
    "for page in data:\n",
    "    quest_gen += page.page_content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\"\"\" Keep the chunk size greater than 1. Hence, Chunk size should be smaller.\"\"\"\n",
    "que_gen_splitter = TokenTextSplitter(model_name= \"gpt-3.5-turbo\", chunk_size=1000, chunk_overlap=200)\n",
    "chunk_ques_gen = que_gen_splitter.split_text(quest_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Covert the string to Document for format to be passed to LLM.\n",
    "Embedding Model doesn't support String.\"\"\"\n",
    "\n",
    "#len(chunk_ques_gen) -- list\n",
    "#type(chunk_ques_gen) -- Str\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "document_ques_gen =  [Document(page_content = ch) for ch in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the LLM. Lower temperature - More authentic and less creative. Higher temperature - More creative\"\"\"\n",
    "\n",
    "\"\"\"The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. \n",
    "An updated version of the class exists in the langchain-openai package and should be used instead\"\"\"\n",
    "#from langchain.chat_models import ChatOpenAI - Deprecated Now\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_ques_gen_ppline = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating interview questions based on the study material and coding material given to you.\n",
    "Your goal is to prepare a coder or a programmer for their exams and coding tests.\n",
    "You do this by asking the questions about the text below:\n",
    "\n",
    "--------\n",
    "{text}\n",
    "--------\n",
    "\n",
    "Create a set of questions that will help the coders and programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = \"\"\"\n",
    "You are an expert at creating interview questions based on the study material and coding material given to you.\n",
    "Your goal is to prepare a coder or a programmer for their exams and coding tests.\n",
    "We have received some questions meeting the expectations upto a certain extent: {existing_answers}\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below\n",
    "\n",
    "--------\n",
    "{text}\n",
    "--------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(input_variables=[\"existing_answers\", \"text\"], template=refine_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
